In computational linear algebra, we would spend a lot of time in matrix equations or expressions like $Ax = b$. However, for large matrices  $A$, it is always difficult and inefficient to find the value of $x$, if we try finding the inverse of $A$ directly. So what we want to do is to "transform" our $A$ into smaller building blocks with specific properties, and use the properties of them to make the equation easier to solve. 

\medskip
\noindent The transformations, should preserve the correctness in matrix calculations (be sufficiently free from truncation errors, as said in the \textbf{Introduction of Chapter 2, master notes}), and be efficient enough to perform. Here, let us talk about the very first transformation, the \textbf{QR Factorisation}. 

\section{QR Factorisation Concept}%
What does \textbf{QR Factorisation} do is to simply decompose an arbitrary matrix $A$, to a unitary matrix $Q$ and an upper triangular matrix $R$. 
i.e. to  write
\[
  A = QR \text{ where } A \in \mathbb{C}^{m\times n}, Q  \text{ unitary } \in \mathbb{C}^{m \times m}, R  \text{ upper triangular } \in \mathbb{C}^{m \times n}
.\]
And without loss of generality we consider $m > n$, and have:
\[
\underset{\begin{array}{c}\\ A \end{array}}%
{
\begin{pmatrix}
  a_{11} & a_{12} & \ldots & a_{1n} \\
  a_{21} & a_{22} & \ldots & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{m1} & a_{m2} & \ldots & a_{mn}
\end{pmatrix}
}
=
\underset{\begin{array}{c}\\ Q \end{array}}%
{
\begin{pmatrix}
  q_{11} & q_{12} & \ldots & q_{1n} & \ldots & q_{1m} \\
  q_{21} & q_{22} & \ldots & q_{2n} & \ldots & q_{2m}\\
  \vdots & \vdots & \ddots & \vdots & \ddots & \vdots\\
  q_{m1} & q_{m2} & \ldots & q_{mn} & \ldots & q_{mm}
\end{pmatrix}
}
\begin{spmatrix}{R}
  r_{11} & r_{12} & \ldots & r_{1n} \\
  0 & r_{22} & \ldots & r_{2n} \\
  0 & 0 & \ldots & r_{3n} \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \ldots & r_{nn} \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \ldots & 0
\end{spmatrix}
\] 
And we called this as the \textbf{complete QR factorisation}. 

\medskip

\noindent Note that, the term $q_{ik}r_{kj}$ would always give out 0 as result when $k > n$ and make no contribution to the value of  $a_{ij}$. Therefore, we could only consider the first $n$ columns of $Q$, $\hat{Q}$ and the first $n$ rows of  $R$,  $\hat{R}$ as result of factorisation instead (i.e. Determine $A = \hat{Q}\hat{R}$ as a reduced alternative).

\subsection*{Exercise 2.3: Find an orthonormal basis of the orthogonal complement of a subspace}
\addcontentsline{toc}{subsection}{Exercise 2.3: Find an orthonormal basis of the orthogonal complement of a subspace}
\subsubsection*{Problem Description}%
\label{ssub:problem_description}

Given a set of vectors $ \{v_1, v_2, \ldots, v_n\}$ spans subspace $U \subset \mathbb{C}^{m}$, we need to find an orthonormal basis of its orthogonal complement $U^{\bot}$, defined as:
\[
U^{\bot} = \{x \in \mathbb{C}^{m}: \forall u \in U, x^{*}u = 0\} 
.\] 
(hint: You should make use of the built in QR factorisation routine

\noindent \texttt{numpy.linalg.qr()}.)

\subsubsection*{What to do}%
\label{ssub:what_to_do}

\begin{enumerate}
  \item Think about what does QR factorisation actually do: what are the properties of $Q$ and $R$  (and similarly $\hat{Q}$ and $\hat{R}$ in the reduced version)?
  \item You might notice $Q$ (and $\hat{Q}$) is unitary, so try to think about the relationship between column vectors in matrix by expanding $Q^*Q = I$.
  \item And the basis of $U$ could be easily seen by the columns of $\hat{Q}$. So if we find the vectors orthogonal to all column vectors in $\hat{Q}$, we should find the basis of the orthogonal complement of $U$.
  \item Think that how we could find the required orthogonal vectors in $Q$. Your task is now to use the relationship you found in (2) to find these vectors, and code with Python.
\end{enumerate}

\newpage
\subsubsection*{Code Implementation}%
\label{ssub:code_implementation}
\lstinputlisting[language=Python, firstline=61, lastline=72]{./python/exercise2.py}

\subsubsection*{Analysis}%
\label{ssub:analysis}
Here we have some explanation to this implementation:
\begin{itemize}
\item Given that we know $Q$ is unitary, we would see all column vectors are orthogonal to each other by expanding
  \[
    [Q^*Q]_{ij} = q_i^*q_j = I_{ij} = \left\{
      \begin{array}{l}
      0 \text{ when $i \neq j$} \\
      1 \text{ when $i = j$}
      \end{array}
    \right.
  .\] 
  \item And since we need to find the vectors that orthogonal to all column vectors in $\hat{Q}$, which is the first $n$ columns of $Q$. We could actually see the remaining $m - n$ columns in  $Q$ are actually orthogonal to all column vectors in $\hat{Q}$, by the mutual orthogonality we observed in column space of $Q$. 
  \item Therefore, what we need to do is just to find the value of Q via \textbf{complete} QR factorisation, via \texttt{np.linalg.qr(A, 'complete')}.
  \item And we need to get the \textbf{last} $m - n$ \textbf{columns} from $Q$, via \texttt{Q[:, n]}. It should be the basis of the orthogonal complementary $U^{\bot}$.
\end{itemize}


