\subsection*{Material Correspondence Table}
I reconstructed the original study material by dividing some parts into smaller sections to make a step-by-step approach of the content in the notes. \smallskip

\noindent If you want to see correspondence of the materials between the original and commentary notes, here is a table indicating the relations: \bigskip

\noindent 
\begin{tabular}{ll}
  Master Notes & Commentary Notes \\
  \hline \\
  1.1. Matrices, vectors and matrix-vector multiplication & Section 1.1 - 1.3 \\
  1.2. Range, nullspace and rank & Section 1.4 - 1.6 \\
  1.3. Invertibility and inverses & Section 1.7  \\
  1.4. Adjoints and Hermitian matrices & Section 1.8 \\
  1.5. Inner products and orthogonality & Section 1.9 \\
  1.6. Orthogonal components of a vector & Section 1.10\\
  1.7. Unitary matrices & Section 1.11  \\
  1.8. Vector norms & Section 1.3\\
  1.9. Projectors and projections &  Section 1.12 \\
  1.10. Constructing orthogonal projectors  & Section 1.13
  \end{tabular}
\newpage
\section{Matrix-Vector Multiplication}%
\textbf{This part might be too easy for most of you - but it would still give you intuition in the contents we would discuss later.}\medskip

\noindent Note that in this course we would discuss \textbf{vectors in \(\mathbb{C}^{n}\) rather than \(\mathbb{R}^{n}\)}, so some vector operations would be different from those you get used to. You would see the details as we going through the course.
\subsection*{Content Checklist}
\noindent In this section we would:
\begin{itemize}
  \item Recap on deriving \textbf{Matrix-Vector multiplication formula}. 
  \item Recap on showing \textbf{linearity} of matrix multiplications. 
  \item \textbf{Implement a Python function} for M-V multiplication.
\end{itemize}
\subsection*{Contents}
Suppose we are given that:
\[
x = \begin{pmatrix} x_1\\x_2\\ \vdots\\ x_n \end{pmatrix} \in \mathbb{C}^{n}
\]
and
\[
  A = \begin{pmatrix} 
  a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn} \\  
\end{pmatrix} \in \mathbb{C}^{m \times n}
.\] 
Here we are asked to consider the result of matrix multiplication $b = Ax$.
\begin{itemize}
  \item The first thing we could confirm is $b \in \mathbb{C}^{m}$ by dimensions of the given matrices.
  \item Then speaking about the $i$th component of $b$, $b_i$ (obviously $1 \le i \le m$). How do we determine the value of $b_i$ in general? 
  \item Normally we get the $i$th row from our matrix $A$, and multiply with the column vector $x$ using matrix multiplication rules:
    \[
    b_i = a_{i1}x_1 + a_{i2}x_2 + \ldots +  a_{in}x_n
    .\]
  \item To generalise it we have
  \[
    b_i = \sum_{j = 1}^{n} a_{ij}x_j, \text{ where } i \in \{1, 2, 3 \ldots m\}
  .\]
  \item And this is what we called Matrix-Vector multiplication. \checked
\end{itemize}
We could then check the map $x \to  Ax$ given by matrix multiplication is a linear map from $\mathbb{C}^{n} \to \mathbb{C}^{m}$. i.e.
  \[
    \forall x, y \in \mathbb{C}^{n}, \alpha \in \mathbb{C}, A(\alpha x + y) = \alpha A x + A y
  .\]
This should be really straightforward.
\subsubsection*{Proof}
\begin{itemize}
    \item Suppose $b = A(\alpha x + y)$, we compute the $i$th entry of $b$,  $b_i = [A(\alpha x + y)]_{i}$ via the formula above:
      \[
        b_i = \sum_{j = 1}^{n} a_{ij}(\alpha x + y)_j =  \sum_{j = 1}^{n} a_{ij}(\alpha x_j + y_j)
      \]
      \[
        = \alpha(\sum_{j = 1}^{n} a_{ij} x_j) + (\sum_{j = 1}^{n} a_{ij} y_{j})
      .\]
    \item Can you see that how could we simplify this expression further? Use the Matrix-Vector formula we derived before:
      \[
        b_i = \alpha (Ax)_i + (Ay)_i
      \]
    \item Therefore, sub in $b_i = [A(\alpha x + y)]_i$ into the equation we have:
      \[
        [A(\alpha x + y)]_i = \alpha (Ax)_i + (Ay)_i
      \]
      \[
        \implies A(\alpha x + y) = \alpha Ax + Ay
      .\]
    \item Therefore, we could now confirm that the map $x \to Ax$ by multiplication is a linear map. \checked
\end{itemize}
You have now seen the derivation of Matrix-Vector Multiplication formula and its property, but how do we implement this schema as a python function? Let's try Exercise 1.1.
\subsection*{Exercise 1.1: Basic M-V Multiplication}
\addcontentsline{toc}{subsection}{Exercise 1.1: Basic M-V Multiplication}
I assumed that all of you have set up the coding environment and obtained the skeleton code of the exercises. If not, see \href{https://comp-lin-alg.github.io/exercises.html}{here} for further instructions.
\subsubsection*{Problem Description}
In this exercise, we are given matrix $A$ and vector $x$ as defined above. What we need to is implementing a function \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises1.basic_matvec}{\texttt{exercises1.basic\_matvec()}}
under the \texttt{cla\_utils} package, which returns the matrix-vector product $b = Ax$. We are asked to implement this with a \textbf{double-nested loop}.

\smallskip
\noindent How do we implement this function? 

\begin{itemize}
  \item Since this is the very first exercise, I would show you the mathematics behind the implementation, the steps you need to do in the implementation, and how to translate steps into code, in details. 
  \item \textbf{However, detailed explanation/analysis about how to write code would only be in this exercise, }in the sense of inspiring you for code implementations instead of just giving you the code.
\end{itemize}
\subsubsection*{What to do}
\begin{enumerate}
    \item Think about the formula we derived in the previous section:
    \[
    b_i = \sum_{j = 1}^{n} a_{ij} x_j, i \in \{1, 2, 3 \ldots m\}
    .\] 
  \item There is a summation sign in the calculation of $b_i$, and it should indicate a \textbf{for} loop in the implementation of calculating a single $b_i$.
  \item \textbf{Note: We always relate the summation in mathematics and for loop in programming together, try to convince yourself that you really understand why we were allowed to do so.}
  \item Also the basic 'algorithm' indicates that we would compute the value of $b_i$ for $m$ times to find the vector \(b\), with the method described in the formula. Therefore, we shall need another \textbf{for} loop to compute all $m$ values of $b_i$. 
  
  Now you should understand why a \textbf{double-nested for loop} is necessary for our implementation.
\end{enumerate}
\newpage
\begin{itemize}
   \item Therefore, the most naive structure of our implementation should be:
\begin{lstlisting}[language=Python]
import numpy as np
def basic_matvec(A, x):
    # Get the dimensions of A
    m, n = ...
    # Initialise vector b as an empty list.
    b = []
    for i in range(m):
        b_i = 0
        # Compute the value of b_i.
        for j in range(n):
            # Apply the basic formula in summation.
            b_i += ...
        # Append the computed b_i to b.
        b.append(b_i)
    return np.array(b)
\end{lstlisting}
\end{itemize}
\subsubsection*{Analysis}
    \begin{itemize}
    \item Then the implementation has been divided into two parts: compute the values of $b_i$ through the inner loop iterations, and add these values to the placeholder, a list $b$ as each inner loop ends.
    \item The second part is very easy to implement, just append the computed $b_i$ to $b$ via \texttt{b.append(b\_i)} at the end of each iteration of the outer loop.
    \item The first part is still straight-forward, but would take more effort in the implementation. To compute the value of $b_i$, we should initialize the value as 0 at first, i.e. \texttt{bi = 0}. Then we could go into the inner loop with counter $j$ to do the summation. 
    \item Then the 'elements' we need in the inner loop is kind of self-intuitive, since it is mentioned in the basic formula:
      \[
        \sum_{i = 1}^{n} \to \texttt{for j in range(n)}
      \] 
       \[
         a_{ij} \to  \texttt{A[i][j]}
      \] 
      \[
        x_j \to  \texttt{x[j]}
      .\]     
    \item Since we are doing summation, what we are going to do in the inner loop is just to keep adding \(a_{ij}x_j\) to \(b_i\)  during iterations of $j$. Think about how you could use code to achieve that.
    \item The function requires an n-dimensional \texttt{ndarray}, so we need to wrap the list \texttt{b} with \texttt{np.array()} when function returns the result.
    \item \textbf{(Important!)} To test your code, do not forget to activate your virtual environment first, and then using \texttt{pytest} to test your code.
    \item \textbf{Check your implementation pass the provided test cases.}
  \end{itemize}
% \subsubsection*{Code Implementation} 
% \begin{itemize}
%     \item Therefore, we have all things we need from analysis above, and fill the template we would get the final implementation:
%       \lstinputlisting[language=Python, firstline=17, lastline=33]{./python/exercise1.py}
%     \end{itemize}
\section{Column-Space Formulation}
\label{ex1.2}
In this section, we would give you an intuition to interpret matrix-vector multiplication as a linear combination of columns of $A$, with coefficients taken from the entries of $x$.
 \begin{itemize}
\item If we write the result of $b = Ax$ explicitly, we would get:
\[
b = \begin{pmatrix} a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\  a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n\\ \vdots\\a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n \end{pmatrix}
\]
\[
 = x_1\begin{pmatrix} a_{11}\\ a_{21} \\ \vdots\\ a_{m1} \end{pmatrix} + x_2 
\begin{pmatrix} a_{12}\\ a_{22} \\ \vdots\\ a_{m2} \end{pmatrix} + \cdots + x_n 
\begin{pmatrix} a_{1n}\\ a_{2n} \\ \vdots\\ a_{mn} \end{pmatrix}
.\]
which is a linear combination with columns of $A$ and entries of $x$.
 \item To simplify this more, we could rewrite matrix $A$ in terms of all of its columns as follows:
    \[
   A = \begin{pmatrix}
     a_1 & a_2 & \ldots & a_n
   \end{pmatrix}
   .\]
   where
   \[
   a_i = \begin{pmatrix} a_{1i}\\ a_{2i} \\  \vdots\\ a_{mi} \end{pmatrix} \in \mathbb{C}^{m}, i \in \{1, 2, \ldots n\} 
   .\]
  \item Therefore, the linear combination should be:
     \[
    b = x_1a_1 + x_2a_2 + \cdots + x_na_n = \sum_{j=1}^{n} x_j a_j
    .\] 
\end{itemize}
\subsection*{Exercise 1.2: M-V Multiplication with C-S Formulation}%
\addcontentsline{toc}{subsection}{Exercise 1.2: M-V Multiplication with C-S Formulation}
\subsubsection*{Problem Description}
The question says the given conditions are unchanged. However, in this implementation of \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises1.column_matvec}{\texttt{exercises1.column\_matvec()}} we were asked to \textbf{use a single loop} over entries of A (and x).

\medskip
\noindent So considering the column-space formulation we mentioned above, we could easily get the implementation by using:
\[
b = Ax =  \sum_{i=1}^{n} x_i a_i
.\]

% \subsubsection*{Code Implementation}
% \lstinputlisting[language=Python, firstline=36, lastline=46]{./python/exercise1.py}
\subsubsection*{Hints} 
Here are several things also worth mentioning:
\begin{itemize}
\item Since we are asked to use a \textbf{single loop} only, we could easily see that using the formula with one $\Sigma$ sign would work. i.e. the formula with column-space formulation.
\item Then according to the formula derivation above, we only need to iterate through the column vectors in matrix $A$, i.e. \texttt{A[:, i]} and multiply with the $i$th entry of $x$, the final result would be calculated as the sum of products in each iterations.
\item The placeholder of the result, \texttt{b}, should not be initialized as an empty array. \textbf{(Think why shouldn't, and what is the correct initializer?)}
% \item That is because in the addition of numpy arrays, two arrays should be equi-dimentioned. Using an empty array would make the dimension zero, which is obviously different from the dimension of product obtained in each iteration.
% \item I used \texttt{for i, elem in enumerate(x)} in my implementation, rather than \texttt{for i in range(len(x))} and \texttt{elem = x[i]} for code simplicity and elegance.
\end{itemize}
\textbf{Remember to check your implementation pass the provided test cases.}
\section{Matrix-Matrix Multiplication}%
\textbf{This part of derivation is similar to the Matrix-Vector case.} Now our matrix $A$ is in dimension $\mathbb{C}^{m \times l}$, and consider another matrix $C \in \mathbb{C}^{l \times n}$ and we have matrix $B = AC$:
\begin{itemize}
  \item Then we think about the entry $(i, j)$ of matrix $B$. To get the value of $b_{ij}$, we should get the $i$th row from $A$ and the $j$th column from $C$, and the calculation goes like this:
    \[
      b_{ij} = a_{i1}c_{1j} + a_{i2}c_{2j} + a_{i3}c_{3j} + \ldots +  a_{il}c_{lj} = \sum_{k = 1}^{l} a_{ik}c_{kj}
    .\] 
    where
    \[
      (i, j) \in \{1, 2, 3 \ldots m\} \times \{1, 2, 3 \ldots n\} 
    .\]
    \item Now we have seen the derivation of formula of matrix multiplication, in different cases.
    \item Then try to apply the column-space formulation to the matrix-matrix multiplication, considering the $i$th column of the result matrix, $b_i$, the column vector is obtained via:
    \[
    b_i = A c_i
    .\] 
    which is another product of matrix-vector multiplication.
    \item Therefore, we expand the matrix-vector multiplication and we could get:
    \[
    b_i = \sum_{j=1}^{l} A_j c_{ij}
    .\]
    for a matrix-matrix multiplication with column-space formulation.

\end{itemize}
\subsection*{Exercise 1.3: Execution Time Comparison}%
\addcontentsline{toc}{subsection}{Exercise 1.3: Execution Time Comparison}
You don't need to implement anything in this exercise, but just run several lines of code in your terminal. \textbf{This exercise is not assessed.}
\subsubsection*{Problem Description}
In this section we are not asked to implement anything, but we could use the defined function $\texttt{time\_matvecs()}$ in package to see the different execution time, for different matrix multiplication algorithm implementations:
\begin{lstlisting}
> basic_matvec(A0, x0) # double-nested loop implementation

> column_matvec(A0, x0) # single loop implementation

> A0 @ x0 # using @ ("at" sign) as matmul operator in numpy
\end{lstlisting}
\subsubsection*{Efficiency Measuring}
Here we compare three implementations mentioned above, and the timing function gives us the following results:
\begin{lstlisting}
>>> import cla_utils
>>> cla_utils.time_matvecs()
Timing for basic_matvec
0.09208189499999975
Timing for column_matvec
0.002095079000000055
Timing for numpy matvec
0.00014622799999841618
\end{lstlisting}
We would clearly see the execution time:
\[
\texttt{nested loop} \gg \texttt{single loop} \gg \texttt{numpy implementation}
.\]
As the master notes saying, in these exercises you should consider the best way to \textbf{make use of Numpy built-in operations} (which will often make the code more maths-like and readable, as well as potentially faster).
\section{Range of a Matrix}%
The master notes have the definition of \texttt{range}, here I would give the definition in set notation \textbf{(alternative version of} \href{https://comp-lin-alg.github.io/L1_preliminaries.html#range-nullspace-and-rank}{Definition 1.4}\textbf{ master notes)}:
\[
\text{range}(A) = \{a : \exists x \in \mathbb{C}^{n}, Ax = a \}, \text{ where } A \in \mathbb{C}^{m \times n}
.\]
Here we would see any $a \in A$ would be expressed as a product in form of M-V multiplication. In other words, we could manipulate $a$ as a product with column-space formulation, with the following interpretation:
\[
a = Ax = \sum_{i=1}^{n} A_i x_i
.\]
Therefore, any element in range($A$) could be demonstrated as a \textbf{linear combination} of \textbf{column vectors} in A. In other words, \textbf{any element $a \in \text{range}(A)$ would be in the vector space spanned by column vectors of $A$.} So we have the following statement:
\[
  \forall a \in \text{range}(A), a \in \text{span}(a_1, \cdots, a_n) \implies \text{range}(A) \subseteq \text{span}(a_1, \cdots, a_n)
.\] 
Also we could see that, \textbf{any linear combination} of columns of $A$ is an element of range$(A)$, which means:
\[
  \forall v \in \text{span}(a_1, \cdots, a_n), v \in \text{range}(A) \implies \text{span}(a_1, \cdots, a_n) \subseteq \text{range}(A)
.\] 
Therefore, we finally have:
\[
  \text{range}(A) = \text{span}(a_1, \cdots, a_n)
.\] 
That is to say, range($A$) is the vector space spanned by the columns of $A$, as mentioned in the  \href{https://comp-lin-alg.github.io/L1_preliminaries.html#range-nullspace-and-rank}{Theorem 1.5} of master notes. 

\section{Null Space and Rank of a Matrix}%
The definition of the null space of an Matrix is very simple as stated by the master notes:
\[
  \text{null}(A) = \{x \in \mathbb{C}^{n}: Ax = 0\} 
.\]
Then we would talk about the rank of a matrix. Actually we have two definitions, \textbf{row rank} and \textbf{column rank}, which refer to the dimension of the row/column space. It could be shown that these two values are equal for all matrices, here is the proof:
\subsubsection*{Proof}
\begin{itemize}
  \item Suppose we have a matrix $A \in \mathbb{C}^{m \times n}$, and we could put this matrix into its RRE form, $A'$, by elementary row operations (an example is given below, where $a'_{in} \neq 1$):
  \[
    A' = \begin{pmatrix}
    1 & a'_{11} & 0 & a'_{14} &  \cdots & 0 & 0 & 0 & \cdots & 0\\
    0 & 0 & 1 & a'_{23} & \cdots & 0& a'_{2n} & 0 & \cdots & 0\\
    0 & 0 & 0 & 0 &  \cdots & 1 & a'_{3n} & 0 & \cdots & 0\\
    \vdots & \vdots & \vdots & \vdots & \vdots &\vdots & \vdots & \vdots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
    \end{pmatrix} 
  .\]
  \item Similarly, we could put $A'$ into its column reduced form,  $A''$. Since the elementary row and column operations are both invertible. That is to say, there exists two invertible matrices  $P$ and  $Q$, such that:
   \[
  QPA = A''
  .\] 
  and since $P$ and $Q$ are invertible, the number of linearly independent row (\textbf{also for column}) vectors in  $A''$ should be equal to the number in $A$.
  \item Now $A''$ is in both row reduced and column reduced form, so $A''$ should be in the following form:
   \[
  A'' = \begin{pmatrix} 
  I_k & 0_{k \times  (n - k)} \\
  0_{(m - k) \times k} & 0_{(m - k) \times (n - k)}
  \end{pmatrix}
  .\]
  \item To be more specific, we could see the structure of $A''$ after the elementary row and column operations through the following figure:
  \begin{figure}[H]
    \centering
    \captionsetup[subfigure]{justification=centering}
      \includegraphics[width=0.5\textwidth]{imgs/rank.png}
      \caption{Structure of \(A\) after elementary row and column operations}
      \label{fig:structure}
  \end{figure}
  \item Then it is not hard to see that, since apart from the identity matrix block $I_k$, the other blocks in $A''$ are just 0s, so the dimension of row space and column space are both equal to the 'size' of the identity matrix block $k$. i.e. the row rank and the column rank are equal for all matrices.
\end{itemize}
Here we have seen the equality of row and column ranks, so as the mater notes saying, we just refer to the rank in this course. By using the definition of range and column space of matrix, for all matrix $A \in \mathbb{C}^{m \times  n}$, we have:
\[
  \text{rank}(A) = \text{dim}(\text{span}(a_1, a_2, \cdots, a_n)) = \text{dim}(\text{range}(A))
.\] 
And I would say the definition of \textbf{full rank} is still straightforward though. However,  to understand it in a more direct (or say 'geometrical') way using the similar intuition from the proof of equality of ranks. So here are two figures to demonstrate of the idea:
\subsubsection*{Demonstration}
\begin{figure}[H]
  \centering
  \captionsetup[subfigure]{justification=centering}
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/rowrank.png}
    \caption{$m \ge n$}
    \label{fig:case1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{imgs/colrank.png}
      \caption{$m < n$}
      \label{fig:case2}
    \end{subfigure}
    \caption{Full rank demonstration}
\end{figure}
\begin{itemize}
  \item Firstly we could think the case $m \ge n$, here $n$ is the 'lesser' in dimension $\mathbb{C}^{m \times n}$, and for the given matrix $A$, the maximum number of linearly independent vectors in column vectors the matrix should be $n$. 
  \item And if we try to continuously do elementary row and column operations like we do in the previous proof, we would get the transformed matrix $A'$. If all column vectors are linearly independent, unlike any general matrix after transformation (see \autoref{fig:structure}), the size of identity matrix block in $A'$ would just fit the 'width' of $A'$($A$ as well), as shown in \autoref{fig:case1}. 
  \item So the rank of matrix $A$, i.e. the size of identity matrix block in $A'$, should be $n$. And the rank cannot be any bigger for all matrix $M \in \mathbb{C}^{m \times n}$. Therefore, we call matrix $A$ has \textbf{full} rank in this case.
  \item In the other case where $m < n$, we could use same intuition to demonstrate the idea of  \textbf{full rank} where the 'lesser' here is the 'height' of $A$, i.e.  $m$ in this case. You could see it from \autoref{fig:case2}.
\end{itemize}
\section{Full Rank and Matrix Properties}%
In this part, we are going to talk about the proof of  \textbf{Theorem 1.9} in the master notes:
\[
  \forall A \in \mathbb{C}^{m \times n}, A \text{ full rank} \iff \neg (\exists x, y \in \mathbb{C}^{n}, x \neq  y \implies Ax = Ay)
\]
\[
  \iff (\forall x, y \in \mathbb{C}^{n}, x \neq y \implies Ax \neq Ay) \iff A \text{ is an injection}
\]
\[
\iff \forall x \in \mathbb{C}^{n}, Ax \text{ is unique in }\mathbb{C}^{m}
.\] 
\subsubsection*{Proof}
\begin{itemize}
  \item Without loss of generality, we consider the case that $m \ge  n$.
  \item \textbf{To prove the forward proposition}, we could use the column-space formulation in multiplication $Ax$ and have:
     \[
    Ax = \sum_{i=1}^{n} x_i a_i
    \]
    with $a_i$ represents the  $i$th column of matrix $A$.
  \item Since $A$ is full rank, so all column vectors are linearly independent. Therefore, $Ax$ gives out a unique linear combination in  $\mathbb{C}^{m}$ formed by columns of $A$. i.e. no two distinct vectors would be mapped to same vector, so the forward direction has been proved.
  \item \textbf{To prove the backward proposition}, we use the Rank-Nullity theorem, for any given linear map $T: V \to  W$, where $V, W$ are vector spaces and  $V$ is finite dimensional:
    \[
      \text{rank}(T) + \text{nullity}(T) = \text{dim}(V)
    .\]
  \item Since $A : \mathbb{C}^{n} \to  \mathbb{C}^{m}$, we have $\text{dim}(A) = n$.
  \item Considering $\text{nullity}(A) = \text{dim}(\text{null}(A))$, since for null$(A)$:
    \[
      \text{null}(A) = \{x \in \mathbb{C}^{n}: Ax = 0_m\} 
    .\]
  \item Since $A$ cannot be a zero matrix, so the only vector could satisfy $Ax = 0_m$ should be  $x = 0_n$. i.e. 
    \[
      \text{null}(A) = \{0_n\} \text{ and nullity}(A) = \text{dim}(\{0_n\}) = 0
    .\]
  \item Therefore, according to Rank-Nullity theorem, we have 
    \[
      \text{rank}(A) = n - 0 = n = \text{min}(m, n)
    .\]
  \item So we could prove A is full rank, and we have done proofs of both forward and backward propositions.
  \item We could do the same thing for the case $m < n$, and just as the master notes saying, the theorem is a consequence of the column space interpretation of matrix-vector multiplication.

\end{itemize}
And also it is clear to see the relations between the full rank and invertibility of an matrix, nothing added for explaining \textbf{Theorem 1.10}.
\subsection*{Exercise 1.11: Construct a Rank-2 Matrix}%
\addcontentsline{toc}{subsection}{Exercise 1.11: Construct a Rank-2 Matrix}
\subsubsection*{Problem Description}
Given that we have $u_1, u_2 \in \mathbb{C}^{m}$ and $v_1, v_2 \in \mathbb{C}^{n}$, we want to construct a rank-2 matrix $A = u_1v_1^{*} + u_2v_2^{*}$. However, from the given code and the specification, we need to implement a function \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises1.rank2}{\texttt{exercises1.rank2()}} returning matrix  $A$ as a product of two other matrices $B$ and $C$.
\subsubsection*{What to do}
We need to consider the following two questions:
\begin{itemize}
  \item How to write $A$ in terms of a product of matrices?
  \item Why $A$ is rank-2 under this construction?
\end{itemize}
Pause at here for a moment to think about these two questions. Feel free to read the answers in the next page if you get confused.
\newpage

\noindent Here we consider the $A$ as a summation of two matrices first:
\[
A = u_1v_1^{*} + u_2v_2^{*}
\]
where
\[
  u_1v_1^{*} = \begin{pmatrix} v_{11}^{*}u_1 & v_{12}^{*}u_1 & \cdots & v_{1n}^{*}u_1 \end{pmatrix} 
\]
and
\[
  u_2v_2^{*} = \begin{pmatrix} v_{21}^{*}u_2 & v_{22}^{*}u_2 & \cdots & v_{2n}^{*}u_2 \end{pmatrix} 
.\] 
Therefore, $A$ could be written in the sum:
\[
  A = u_1v_1^{*} + u_2v_2^{*} = \begin{pmatrix} v_{11}^{*}u_1 + v_{21}^{*}u_2 & v_{12}^{*}u_1 + v_{22}^{*}u_2 & \cdots & v_{1n}^{*}u_1 + v_{2n}^{*}u_2 \end{pmatrix} 
\]
with
\[
  a_{i} = v_{1i}^{*}u_1 + v_{2i}^{*}u_2 = \begin{pmatrix} u_1 & u_2 \end{pmatrix} \begin{pmatrix} v_{1i}^{*}\\ v_{2i}^{*} \end{pmatrix}
.\] 
To generalize this, we could now write $A$ in terms of a product of two matrices:
\[
A = \begin{pmatrix} u_1 & u_2 \end{pmatrix} \begin{pmatrix} v_{1}^{*}\\ v_{2}^{*} \end{pmatrix}   = \begin{pmatrix} u_1 & u_2 \end{pmatrix} \begin{pmatrix} v_{1}^{*} & v_{2}^{*} \end{pmatrix}^{\top}
.\]
So our first job has been done, now it is your time for implementation. \medskip

\noindent 
\textbf{Remember to check your implementation pass the provided test cases.}
% \subsubsection*{Code Implementation}
% \lstinputlisting[language=Python, firstline=56, lastline=69]{./python/exercise1.py}
\subsubsection*{Analysis}
Now let us talk about the rank of $A$, why it is necessarily 2?
We recall the result of $A$ we have found with column-space formulation: 
\[
A = u_1v_1^{*} + u_2v_2^{*} = \begin{pmatrix} v_{11}^{*}u_1 + v_{21}^{*}u_2 & v_{12}^{*}u_1 + v_{22}^{*}u_2 & \cdots & v_{1n}^{*}u_1 + v_{2n}^{*}u_2 \end{pmatrix} 
.\]
We could see that, every column of $A$ is a linear combination of $u_1$ and $u_2$, assume that they are linearly independent. Therefore, when we are talking about the range of $A$, it should be a vector space spanned by $u_1$ and $u_2$ and the dimension of the space should be 2. \textbf{i.e. the rank is 2 obviously.} 
\newpage
\section{Invertibility and inverses}%
The idea and properties related to invertibility and inverses should be quite familiar, I would not talk about this here. What I want to focus is the proof and the coding exercise in this part.
\subsubsection*{Problem Description}
The exercise is asked us to prove that, there exists a unique \textbf{left} inverse $Z$ of a given square matrix $A \in \mathbb{C}^{n \times n}$ such that $ZA = I_n$. And here is the proof:
\subsubsection*{Proof}
\begin{itemize}
  \item Suppose there exists another left inverse $Z'$ of $A$, such that  $Z'A = I_n$ but  $Z' \neq  Z$.
  \item Consider the $(i, j)$ entry of the product $Z'A$, if $i \neq j$:
    \[
      (Z'A)_{ij} = \sum_{k=1}^{n} z'_{ik}a_{kj} = 0
    \]
    and similarly
    \[
      (ZA)_{ij} = \sum_{k=1}^{n} z_{ik}a_{kj} = 0
    .\]
  \item Therefore, we subtract one equation from another, we would have:
    \[
      (ZA)_{ij} - (Z'A)_{ij} = \sum_{k=1}^{b} (z_{ik} - z'_{ik})a_{kj} = 0 - 0 = 0
    .\]
  \item The sum equals zero indicates that, when $i \neq  j$, for all $k$, the non-trivial case would give us:
     \[
    z_{ik} - z'_{ik} = 0 \implies z_{ik} = z'_{ik}
    .\] 
  \item Similarly, when $i = j$, we would know that  $(Z'A)_{ij} = (ZA)_{ij} = 1$. So after subtraction like we did above, we would get:
    \[
      (ZA)_{ij} - (Z'A)_{ij} = \sum_{k=1}^{b} (z_{ik} - z'_{ik})a_{kj} = 1 - 1 = 0
    .\]
    and again
    \[
    z_{ik} - z'_{ik} = 0 \implies z_{ik} = z'_{ik}
    .\] 
  \item Therefore, we have seen $z_{ik} = z'_{ik}$ in each scenario, therefore, our suggested new left inverse $Z'$ is just original $Z$ itself, which gives a contradiction.
   \item So we have now proved the left inverse is unique.
  \item We could use the same strategy to prove the uniqueness of right inverse $Z$ such that  $AZ = I_n$.
  \item Therefore, we could say the inverse $Z$, of a square matrix $A$, such that  $ZA = AZ = I_n$, is unique if the matrix itself is invertible.
\end{itemize}
\subsection*{Exercise 1.13: Find the inverse of a given matrix}%
\addcontentsline{toc}{subsection}{Exercise 1.13: Find the inverse of a given matrix}
\subsubsection*{Problem Description}
Given that a square matrix $A = I_m + uv^{*} \in \mathbb{C}^{m \times m}$, where $u, v \in \mathbb{C}^{m}$. Suppose we could find the inverse $A'$ written in  $A' = I_m + \alpha uv^{*}$, determine the value of $\alpha$ and implement the function \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises1.rank1pert_inv}{\texttt{exercises1.rank1pert\_inv()}} to return the inverse matrix $A'$. \medskip

\noindent Pause at here for a moment to think about writing value of \(\alpha\) in terms of \(u\) and \(v\). Feel free to read the answers in the next page if you get confused.
\newpage
\subsubsection*{What to do}
\noindent So we try to determine the value of $\alpha$ first:
\begin{itemize}
  \item Given that $A A^{-1} = I_m$:
    \[
      A A^{-1} = (I_m + u v^{*})(I_m + \alpha u v^{*}) = I_m + uv^{*} + \alpha uv^{*} + \alpha u v^{*}uv^{*} = I_m
    .\]
  \item From the numerical analysis course we know the inner product, $v^{*}u$ is a scalar, then we could have:
    \[
      uv^{*} + \alpha uv^{*} + \alpha (v^{*}u)uv^{*} = 0_{m \times m} \implies (1 + \alpha + \alpha v^{*}u)uv^{*} = 0_{m \times m}
    .\]
  \item Therefore, we have:
    \[
    \alpha (1 + v^{*}u) = -1 \implies \alpha = -\frac{1}{1 + v^{*}u}
    \]
    and
    \[
    A^{-1} = I_m -\frac{1}{1 + v^{*}u} uv^{*}
    \]
    whenever $A$ is invertible.
\end{itemize}
So the code implementation for you should be very simple. \medskip

\noindent \textbf{Check your implementation pass the provided test cases.}
\subsubsection*{Efficiency Measuring (Non-assessed part for this exercise)}
To measure the time taken for computing inverse of this implementation, and compare it with the built-in numpy implementation (just like what we did for timing the matrix-vector multiplication), I add the following code:
\lstinputlisting[language=Python, firstline=65, lastline=77]{./python/exercise1.py}
with pre-defined \texttt{u0}, \texttt{v0}, and the output shows that:
\begin{lstlisting}
>>> import cla_utils
>>> cla_utils.time_matinvs()
Timing for basic_matinv
0.0013024520000044504
Timing for numpy matinv
0.005589449000012792
\end{lstlisting}

\noindent We could see that the numpy implementation is slower. The reason of this output is that, unlike our implementation, the numpy implementation didn't take the advantage of structure of \(A\) - a rank-2 matrix constructed via \(u\) and \(v\), therefore, the complexity using numpy is greater than our implementation and the time taken is indeed longer.

\section{Adjoints and Hermitian Matrices}%

We firstly skip the definition of adjoints and Hermitian matrices since they are straightforward enough. Then we see the proof of \href{https://comp-lin-alg.github.io/L1_preliminaries.html#adjoints-and-hermitian-matrices}{Theorem 1.15}, which states:
\[ 
  (AB)^{*} = B^{*}A^{*}, A \in \mathbb{C}^{m \times n}, B \in \mathbb{C}^{n \times l}
.\] 
And the proof should be simple.
\subsubsection*{Proof}
\begin{itemize}
  \item Considering the $(i, j)$ entry of $(AB)^{*}$ and we have:
    \[
      (AB)^{*}_{ij} = \overline{(AB)_{ji}}=\overline{\sum_{k=1}^{n} a_{jk}b_{ki}} = \sum_{k=1}^{n} \overline{a_{jk}b_{ki}} = \sum_{k=1}^{n} \overline{a_{jk}}\overline{b_{ki}}
    \]
    \[
      = \sum_{k=1}^{n} (a^{*})_{kj} (b^{*})_{ik} = \sum_{k=1}^{n} (b^{*})_{ik} (a^{*})_{kj} = (B^{*}A^{*})_{ij}
    .\]
  \item Therefore, we finally have $(AB)^{*} = B^{*}A^{*}$.
\end{itemize}
And then we could look at the following exercise implementing the required function \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises1.ABiC}{\texttt{exercises1.ABiC()}}.
\subsection*{Exercise 1.16: Hermitian Matrices and Multiplication}%
\addcontentsline{toc}{subsection}{Exercise 1.16: Hermitian Matrices and Multiplication}
This exercises have several mini-questions for us to solve. The first part is mainly about proof with hermitian matrices, and the function implementation part is about the application. Let's see the proof exercise first.

\subsubsection*{Task 1}%

\noindent Given that $A = B + iC \in \mathbb{C}^{m \times m}$ where $B, C$ are both real matrices with compatible dimensions, and  $A$ is hermitian, show that:
\[
  B = B^{\top} \text{ and } C = -C^{\top}
.\]
The answer is in the next page.
\newpage
\subsubsection*{Proof}
\begin{itemize}
  \item Given that $A$ is hermitian then the property tells us:
    \[
      A = A^{* } \text{ with } a_{ij} = (a^{*})_{ij} = \overline{a_{ji}}
    .\]
  \item Considering the $ (i, j)$ and $(j, i)$ entries of $A$, we have:
    \[
    a_{ij} = b_{ij} + ic_{ij} \text{ and } a_{ji} = b_{ji} + ic_{ji}
    .\] 
  \item Therefore, since $A$ is hermitian, we have:
    \[
    a_{ij} = \overline{a_{ji}} \implies b_{ij} + ic_{ij} = \overline{b_{ji} + ic_{ji}} = b_{ji} - ic_{ji}
    .\] 
  \item Clearly
    \[
    b_{ij} = b_{ji} \text{ and } c_{ij} = -c_{ji}
    .\]
  \item Therefore,
    \[
    B = B^{\top} \text{ and } C = -C^{\top}
    .\] 
\end{itemize}
\subsubsection*{Task 2}%
We required to compute the real and imaginary part of the matrix multiplication product $z = Ax$, where  $A$ is defined above. To save memory the function would accept the contents of $A$ as an argument, but a modified matrix $\hat{A}$, where
\[
\hat{A}_{ij} = \left\{
  \begin{array}{l}
  B_{ij} \text{ where } i \ge j \\
  C_{ij} \text{ where } i < j.
  \end{array}
\right.
.\] 
Therefore, given a matrix $\hat{A}$ and $x_r, x_i$ where  $x = x_r + ix_i$, we need to implement the function \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises1.ABiC}{\texttt{exercises1.ABiC()}} to get the real and imaginary parts of \(z = Ax\). \medskip

\noindent Note: you should use \textbf{column-space formulation only} when multiplying matrices and vectors. If you forget about the column-space formulation, check the exercise \ref{ex1.2}.\medskip

\noindent 
Pause at here for a moment to think about your implementation step by step. Feel free to read the instructions in the next page if you get confused.
\newpage
\subsubsection*{What to do}
\begin{enumerate}
\item Since we don't have contents of $A = B + iC$ as required, we could firstly try to get contents in $B$ and $C$ from $\hat{A}$.
\item The contents in $\hat{A}$ should look like this:
  \[
    \hat{A} = \begin{pmatrix} 
      b_{11} & c_{12} & c_{13} & \ldots & c_{1m} \\
      b_{21} & b_{22} & c_{23} & \ldots & c_{2m} \\
      b_{31} & b_{32} & b_{33} & \ldots & c_{3m} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\ 
      b_{m1} & b_{m2} & b_{m3} & \ldots & b_{mm}
    \end{pmatrix} 
  .\]
  So what we could do is to extract the lower-triangular part of $B$ and upper-triangular part of $C$ from $\hat{A}$, by using \texttt{np.triu()} and \texttt{np.tril()}, and then get the full form of $B$ and  $C$ by transposition.
 \item After having $B$ and $C$, we could simply calculate the value of  $z = Ax$:
    \[
      z = Ax = (B + iC)(x_r + ix_i) = (Bx_r - Cx_i) + i(Bx_i + Cx_r)
   \]
   where $z_r$ and $z_i$ could be easily seen.
\end{enumerate}
\textbf{Remember to check your implementation pass the provided test cases.}

\section{Inner Products and Orthogonality}%
The idea of these two concepts are very simple but they are extremely important in computational linear algebra. We would leave the proof of showing inner map is bilinear as an exercise here.
\newpage
\subsubsection*{Proof}%
\begin{itemize}
  \item Considering $f(x, y) = x^{*}y$, we firstly show the inner map is linear respect to $x$ by deriving $f(\alpha x + x', y)$:
    \[
    f(\alpha x + x', y) = (\alpha x + x')^{*}y = \sum_{i=1}^{n} \overline{(\alpha x + x')_i}y_i
    \]
    \[
      = \sum_{i=1}^{n} (\alpha \overline{x}_i + \overline{x'}_i)y_i = \alpha \sum_{i=1}^{n} \overline{x}_{i} y_i + \sum_{i=1}^{n} \overline{x'_i} y_i
    \]
    \[
      = \alpha x^{*}y + x'^{*}y = f(\alpha x, y) + f(x', y)
    .\] 
  \item Clearly the inner map is linear with respect to  $x$, and similarly the part about $y$ could be shown in a similar way.
  \item Therefore we could see the inner map is bilinear.
\end{itemize}

\bigskip

\begin{center}
  \textit{\large End of Week 1}
\end{center}
\newpage
\section{Orthogonal Components of a Vector}%
Given that we have a orthonormal set of vectors $S = \{q_1, q_2, \ldots, q_n\} $ where $q_i \in \mathbb{C}^{m}$, and for any vector $v \in \mathbb{C}^{m}$, we have:
\[
  v = r + \sum_{i=1}^{n} (q_iq_i^{*})v
.\] 
More specifically, it also shows that:
\begin{itemize}
  \item Given that we have an orthonormal set(basis) $S$, we could always split this vector to a linear combination of vectors in the orthonormal set, and a secret vector $r$. 
  \item This secret vector $r$ orthogonal to any element in $S$.
  \item So if we add $r$ to $S$ and gives out a new set $S'$, the newly formed set is still necessarily an orthonormal basis.
  \item Can we use this idea in finding an orthonormal basis from any basis?
\end{itemize}
We would discuss this later in next chapter for $QR$ decomposition.
\subsection*{Exercise 1.20: Orthonormal Decomposition of Vectors}
\addcontentsline{toc}{subsection}{Exercise 1.20: Orthonormal Decomposition of Vectors}
\subsubsection*{Problem Description}%
Given a vector $v$ and an orthonormal set $Q$, we were asked to write $v$ as a sum of linear combination of elements in $Q$ \textbf{and} the residual vector $r$.

\medskip
\noindent The function \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises2.orthog_cpts}{\texttt{exercises2.orthog\_cpts()}} would do this work. It returns the coefficients $u$ in the linear combination, and the residual vector $r$. Your work is to implement this function.\subsubsection*{What to do}%
\begin{enumerate}
\item Recall that
  \[
    v = r + \sum_{i=1}^{n} (q^{*}_i v)q_i
  .\]
  We know that the coefficients $u_i$ comes from the inner product between $q_i$ and $v$ where $u_i = q^{*}_iv$.
\item And what we need to do is just keep subtracting compoents of $q_i$ from $v$ (should be $u_iq_i$), until all orthogonal components are removed from  $v$, then we would get our  $r$.
\item Think about it: can you get \(u, r\) without using loops? (You might use matrix multiplications, i.e. "vectorized" operations instead.)
\item Note that implementations with \textbf{too many loops} may get your \textbf{mark down} during marking process. Sometimes using only one loop might be counted as "too much" as well.
\end{enumerate} 
\textbf{Remember to check your implementation pass the provided test cases.}
\section{Unitary Matrices}%
The definition, theorem and proof stated in the master notes in this section is clear enough, and I would like to emphasize one useful trick dealing with unitary matrices:
\[
Q \text{ is unitary } \iff Q^{*} = Q^{-1} \iff I = Q^{*}Q 
.\] 
and $Q^{*} = Q^{-1}$ could be interpreted as a change of orthogonal basis.
\subsection*{Exercise 1.23: Solve the System $Qx=b$}
\addcontentsline{toc}{subsection}{Exercise 1.23: Solve the System $Qx=b$ with Unitary Matrices}
\subsubsection*{Problem Description}%
Given a square \textbf{unitary} matrix $Q$ and a vector $b$, find the vector $x$ satisfies $Qx = b$ without using inverses of $Q$ in the implementation of \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises2.solveQ}{\texttt{solveQ()}}.

\subsubsection*{What to do}%
\begin{enumerate}
\item Since we are not allowed to use inverses, and $Q$ is unitary, as we stated above, we could replace $Q^{-1}$ by $Q^{*}$ then $x = Q^{-1}b = Q^{*}b$.
\item Then $Q^{*}$ could be easily represented as the combination of transposing the conjugate of matrix $Q$ via \texttt{numpy} operations.

(Hint: try to consider \texttt{numpy.conj()} and \texttt{Q.T} as transpose of \texttt{Q}).
\item Then we could simply get $x = Q^{*}b$ and implement the function.
\end{enumerate}
\textbf{Remember to check your implementation pass the provided test cases.}

\subsubsection*{Efficiency Measuring (Non-assessed part for this exercise)}%
To test the performance of function efficiency for random matrices with different sizes (compared with the general purpose \texttt{numpy.linalg.solve()}), we have the following code:
\lstinputlisting[language=Python, firstline=17, lastline=31]{./python/exercise2.py}
Here I pass an \texttt{lambda} (i.e. a "function call" with explicit arguments \texttt{Q} and \texttt{b}) to the \texttt{timeit.Timer} constructor rather than an pre-defined helper (timeable) function for code simplicity, and I can also pass variables to either \texttt{solveQ()} and \texttt{numpy.linalg.solve()} as I want.

\medskip
\noindent Since we use adjoint matrices rather than inverses, which reduces a certain amount of operations in finding inverses, so I would expect \texttt{solveQ()} would take less time than general purpose \texttt{solve()}. And the python console gives me the following result:

\bigskip
\begin{lstlisting}
>>> import cla_utils
>>> cla_utils.time_solveQ()
--- Input matrix size n = 100 ---
Timing for solveQ
0.00011220100000031152
Timing for general purpose solve
0.0003564039999996993
--- End for testing matrix with n = 100 ---
--- Input matrix size n = 200 ---
Timing for solveQ
0.00024624399999950697
Timing for general purpose solve
0.0006074579999992835
--- End for testing matrix with n = 200 ---
--- Input matrix size n = 400 ---
Timing for solveQ
0.0007872969999986879
Timing for general purpose solve
0.002527909000001216
--- End for testing matrix with n = 400 ---
\end{lstlisting}
We could easily see that our implementation is more efficient than the general purpose \texttt{solve()}, given that the matrix is unitary.

\medskip
\noindent Note: The explanation of vector norms in \href{https://comp-lin-alg.github.io/L1_preliminaries.html#vector-norms}{master notes 1.8} is clear enough so I would not cover this in my commentary.

\section{Projectors and Projections}%
The concept of projector is self-intuitive, but the relations between projector $P$ and its complementary projector are still worth mentioning.

\medskip
\noindent Let's consider a projector $P$ first:
\begin{itemize}
  \item Suppose $\exists x: Px=v$, i.e. $v \in \text{range}(P)$, we could see that:
    \[
      Pv = P(Px) = P^2x = Px = v
    .\]
    we could see projector $P$ doesn't change vectors in its range. (see \autoref{inrange}).
  \item Suppose $v \not\in \text{range}(P)$, considering the vector $Pv - v$:
     \[
       P(Pv - v) = P^2v - Pv = Pv - Pv = 0
    .\]
    Here we could say $Pv - v$ is the nullspace of $P$, shown in \autoref{outrange}.
  \begin{figure}[H]
    \centering
  \captionsetup[subfigure]{justification=centering}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{imgs/inline.png}
    \caption{$v \in \text{range}(P)$}%
    \label{inrange}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{imgs/outrange.png}
      \caption{$v \notin \text{range}(P)$}
      \label{outrange}
    \end{subfigure}
    \caption{Projecting $v$ with projector $P$}
  \end{figure}
\end{itemize}
Then consider the \textbf{complementary projector} $I - P$:
\begin{itemize}
  \item Obviously it's a projector as well. i.e. $(I - P)^2 = I - P$, and simply
    \[
      Pu = 0 \implies (I - P)u = u - Pu = u
    .\]
  \item The proposition above shows that, if a vector $u \in \text{null}(P)$, it is in $\text{range}(I - P)$. To generalize this, we have:
    \[
      [\forall u \in \text{null}(P) \implies \text{range}(I - P)] \implies
    \]
    \[
      \text{null}(P) \subseteq \text{range}(I - P)
    .\] 
    In other words, the nullspace of $P$ is contained in the range of $I - P$.
  \item Suppose there exists an vector $v \in \text{range}(I - P)$, in other words
    \[
      \exists w \in \mathbb{C}^{m}: v = (I - P)w = w - Pw
    .\]
  \item Then it shows that
    \[
      Pv = P(w - Pw) = Pw - P^2w = Pw - Pw = 0
    .\] 
  \item So indeed $v \in \text{null}(P)$, in this direction we could have:
    \[
      [v \in \text{range}(I - P) \implies v \in \text{null}(P)] \implies
    \]
    \[
      \text{range}(I - P) \subseteq \text{null}(P)
    .\]
    \item And this leads to the result
      \[
        \text{null}(P) = \text{range}(I - P)
      .\]
      similarly
      \[
        \text{null}(I - P) = \text{range}(P)
      .\] 
\end{itemize}
Important things for projector $P$: 
\begin{itemize}
\item  It separates  $\mathbb{C}^{m}$ into two spaces, range($P$) and null($P$), where 
  \[
  \text{range}(P) \cap \text{null}(P) = \{\textbf{0}\}
  .\] 
  The reason of why the intersection contains $\textbf{0}$ only is that, considering the two sets $\text{null}(P)$ and $\text{null}(I - P)$, the only common element is $\textbf{0}$. That is because any vector $v$ in both sets satisfies $v = v - Pv = (I - P)v = 0$. And since we know $\text{null}(I - P) = \text{range}(P)$, the results could be shown as stated above. 

\item Conversely, if there exists two subspaces $S_1$ and $S_2$ of $\mathbb{C}^{m}$, where $S_1 \cap S_2 = \{\textbf{0}\}$, there exists a projector $P$ whose range is $S_1$ and nullspace is $S_2$.
\end{itemize}
  \subsubsection*{Proof}%
\begin{itemize}
  \item The proposition above states that, any vector in $\mathbb{C}^{m}$ could be split into two components, in subspaces $S_1$ and $S_2$ respectively.
  \item Therefore, to prove this proposition, we could simplify this problem to the following proposition:
    \[
    \text{Given that } S_1 \cap S_2 = \{0\}, \forall v \in \mathbb{C}^{m}, \text{ find } v_1 \in S_1, v_2 \in S_2: v_1 + v_2 = v 
    .\]
    where the projection $Pv$ gives $v_1$ and the complementary projection $(I - P)v$ would gives $v_2$.
  \item Obviously we could find the vector pair, since these two subspaces are distinct. But is the vector pair $(v_1, v_2)$ unique?
  \item Suppose there exists another vector pair $(v_1', v_2') \in S_1 \times S_2$ which satisfies the conditions above, where clearly $v_i \neq v_i'$ and:
    \[
      \begin{array}{l}
      v_1 + v_2 = v \\
      v_1' + v_2' = v
      \end{array}
    .\] 
\item Subtract the first equation by the second, and we get:
  \[
    (v_1 - v_1') + (v_2  - v_2') = \textbf{0} 
  .\] 
\item Say $v_1'' = v_1 - v_1'$ and $v_2'' = v_2 - v_2'$, and we could clearly see $v_i'' \in S_i$ due to the properties of subspaces. And we could get:
  \[
  v_1'' + v_2'' = \textbf{0} \implies v_1'' = -v_2''
  .\]
 \item We know that the subspace preserves scalar multiplication, therefore, we could see $v_1''$ is in $S_2$ and  $v_2''$ is in $S_1$ as well.
  \item In this case, we know that $v_1'', v_2''$ are both in $S_1, S_2$. So we could interpret $v_1'', v_2'' \in S_1 \cap S_2 = \{\textbf{0}\} $.
  \item Then obviously $v_1'' = v_2'' = \textbf{0}$, and
    \[
      \begin{array}{l}
      v_1'' = v_1 - v_1' = \textbf{0} \\
      v_2'' = v_2 - v_2' = \textbf{0} \\
      \end{array}
    \implies 
      \begin{array}{l}
      v_1 = v_1' \\
      v_2 = v_2'
      \end{array}
    .\]
    which contradicts our assumption $v_i \neq v_i'$, so the vector pair $(v_1, v_2)$ should be unique.
    \item Therefore, we could find a unique decomposition of $v$ into two components $v_1, v_2$ in  $S_1$ and $S_2$. 
    \item Since we take $v$ arbitrarily, and the components  $v_1, v_2$ in corresponding subspaces $S_1, S_2$ could be written as $Pv$ and  $(I - P)v$ for all chosen $v$, (which are both \textbf{bijection}). So we could say there exists a projector  $P$ such that range$(P) = S_1$ and null$(P) = S_2$.
\end{itemize}


\section{Orthogonal Projectors and Construction}%
We have known the definition of projectors, but how about the orthogonal projectors? The definition states that a projector $P$ is orthogonal if:
\[
\forall v \in \mathbb{C}^{m}, (Pv)^{*}(Pv - v) = 0
.\] 
This definition might still be too abstract, so we put the definition into the following \autoref{orthogproj}:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/orthogonal_proj.png}
    \caption{Projecting $v$ with Orthogonal Projector $P$}
    \label{orthogproj}
  \end{figure}

\noindent We could see the projector $P$ splits $v$ into two components,  $Pv$ and  $Pv - v$. However, in this case these two components are orthogonal to each other, and computationally, their inner product should be 0. That's where the definition of  \textbf{orthogonal} projector comes from. 

\noindent \textbf{Note:} More specifically, an orthogonal projector splits $\mathbb{C}^{m}$ into two orthogonal subspaces, range$(P)$ and null$(P)$.

\bigskip
\noindent Now we know the definition of an orthogonal projector, but how do we construct an orthogonal projector? \textbf{In this course, we are introduced to construct from a set of orthonormal vectors.} 
\begin{itemize}
\item From the knowledge we get in \href{https://comp-lin-alg.github.io/L1_preliminaries.html#constructing-orthogonal-projectors-from-sets-of-orthonormal-vectors}{section 1.10} master notes, we know that, given we have a set of orthonormal vectors $\{q_1, q_2, \ldots, q_n\}$, for all vector $v \in \mathbb{C}^{m}$, we could write $v$ in terms of:
  \[
    v = r + \sum_{i = 1}^{n} (q_iq_i^{*})v
  .\] 
\item And if we write matrix $\hat{Q}$ as:
  \[
    \hat{Q} = \begin{pmatrix} q_1 & q_2 & \ldots & q_n \end{pmatrix} 
  .\]
  we could see $\sum_{i=1}^{n} (q_iq_i^{*})v$ is in the column space of $\hat{Q}$, and the component $r$ in $v$ should be orthogonal to any vectors in the orthonormal set $ \{q_i\} $, i.e. $r$ should be orthogonal to the column space of $\hat{Q}$.
  \item So if we consider $P = \sum_{i=1}^{n} (q_iq_i^{*})$, then we have:
    \[
    v = r + Pv
    .\] 
    and $r$ and $Pv$ should be orthogonal to each other, as we stated above. So in this case, we have $P$ as our orthogonal projector.
\end{itemize}
So now we know how to construct an orthogonal projector, but what the form of an orthogonal projector usually looks like?
\[
P \text{ orthogonal } \implies P = \hat{Q}\hat{Q}^{*}
.\] 
\subsubsection*{Proof}%
\begin{itemize}
\item First of all, computationally if we expand the multiplication $\hat{Q}\hat{Q}^{*}$ :
  \[
    \hat{Q}\hat{Q}^* = \begin{pmatrix} q_1 & q_2 & \ldots & q_n \end{pmatrix} \begin{pmatrix} q_1^{*}\\ q_2^{*} \\\vdots\\ q_n^{*} \end{pmatrix} = q_1q_1^* + q_2q_2^* + \ldots + q_nq_n^{*} =  \sum_{i=1}^{n} (q_iq_i^{*})
  .\]
  which is exactly what we see in the construction of the orthogonal projector $P$. \textbf{Personally I think all of you should understand the proof at this level.}
\item Also, in the light of change of basis, the product $\hat{Q}^*v$ gives the coefficients of $v$ after projecting along the columns of  $\hat{Q}$, and the multiplication of $\hat{Q}$ on $\hat{Q}^*v$ gives the projections of $v$ expanded in the canonical basis. 
  \item Therefore, the multiplication by $\hat{Q}\hat{Q}^*$ does the same job as the orthogonal projector $P$ does. We could finally confirm that the orthogonal projectors has a simple form  $P  = \hat{Q}\hat{Q}^{*}$. 
  \item \textbf{Note:} the last two parts of the proof are mentioned under \textbf{Theorem 1.28 master notes}, and I think it is not necessary to memory by heart but would give you a deeper understanding if digest them.
\end{itemize}
Since any orthogonal projector has the form $P = \hat{Q}\hat{Q}^*$, we have the following properties of orthogonal projectors to discuss:
\subsubsection*{Property 1}%

\begin{itemize}
\item We have known that the orthogonal projector $P$ gives an orthogonal projection to range of $P$. 
\item However, from our interpretation in the previous proof, the orthogonal projector $\hat{Q}\hat{Q}^*$ would eventually expands any $v$ by columns of $\hat{Q}$. 
\item So we could conclude that, $P = \hat{Q}\hat{Q}^*$ is an orthogonal projection to range of $\hat{Q}$.
\item Moreover we could say $\text{range}(P) = \text{range}(\hat{Q})$, since the orthogonal projection on these two planes are equivalent.
\end{itemize}
\subsubsection*{Property 2}%
\begin{itemize}
\item Similarly, the complementary projector $P_{\bot} = I - \hat{Q}\hat{Q}^*$ would give a orthogonal projection to nullspace of $\hat{Q}$.
\item And clearly $\text{null}(P) = \text{null}(\hat{Q})$.
\end{itemize}
\subsubsection*{Property 3}%
\begin{itemize}
  \item Given that any orthogonal projector has the form $P = \hat{Q}\hat{Q}^*$, we could see its adjoint $P^* = (\hat{Q}\hat{Q}^*)^* = (\hat{Q}^*)^*(\hat{Q})^* = \hat{Q}\hat{Q}^* = P$. 
  \item And we have the following theorem:
    \[
    P^* = P \iff P  \text{ orthogonal}
    .\] 
\end{itemize}
\subsubsection*{Proof}%
\begin{itemize}
\item The proof of the backward proposition has been shown above, and we need to prove the forward one: $P^* = P \implies P$ orthogonal.
\item We choose a vector $v \in \mathbb{C}^{m}$ arbitrarily, and we have:
  \[
    (Pv)^{*}(Pv - v) = v^*P^*(P - I)v = v^*P(P - I)v 
  \]
  \[
  = v^*(P^2 - P)v = v^*(P - P)v = 0 
  .\] 
  Therefore $P$ is orthogonal.
\end{itemize}

\subsection*{Exercise 1.30 Constructing Orthogonal Projectors}
\addcontentsline{toc}{subsection}{Exercise 1.30 Constructing Orthogonal Projectors}
\subsubsection*{Problem Description}%
This exercise is a relatively easy one - given an orthonormal set of vectors $Q$, we need to implement a function \href{https://comp-lin-alg.github.io/cla_utils.html#cla_utils.exercises2.orthog_proj}{\texttt{exercises2.orthog\_proj()}} to compute an orthogonal projector $P$, which projects vectors to the subspace spanned by elements of  $Q$.
\subsubsection*{What to do}%
We just need to use the construction of orthogonal projector $P = \hat{Q}\hat{Q}^*$, and in this exercise we can easily use our provided $Q$ as the orthonormal matrix. The code implementation should be straightforward enough. \medskip

\noindent \textbf{Remember to check your implementation pass the provided test cases.}
\subsubsection*{What happens next}%
\noindent After learning all the basic things in this chapter, the exited things finally would come in the following chapters \ldots! Next chapter would be based on the knowledge of orthogonal projectors and we would talk about \textbf{QR factorisation}. 

\bigskip

\begin{center}
  \textit{\large \st{End of Week 1 (we haven't finish this week yet!)}} \\
  \medskip
  \textit{\large End of Chapter 1}
\end{center}




